{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "80a37b3c",
      "metadata": {},
      "source": [
        "# PyTorch 기초 학습 (Week 1)\n",
        "\n",
        "## 학습 목표\n",
        "- PyTorch 설치 상태와 MPS(GPU) 사용 가능 여부를 확인한다.\n",
        "- Tensor 생성/연산/변환의 기본 흐름을 이해한다.\n",
        "- NumPy와의 차이, Java 배열과의 차이를 비교한다.\n",
        "\n",
        "## Java 개발자 관점의 비유\n",
        "- Tensor ≈ Java 배열 + 벡터 연산 + GPU 가속 + 자동 미분(autograd)\n",
        "- NumPy array ≈ Java 배열 + 벡터 연산 (GPU/자동 미분 없음)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "752ac446",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 1. PyTorch 설치 확인 및 MPS 사용 가능 여부\n",
        "# =============================================================================\n",
        "# 학습 목표: torch 버전과 MPS(GPU) 지원 여부를 확인한다\n",
        "# Java 비유: 런타임 버전과 하드웨어 가속 옵션 확인과 유사\n",
        "\n",
        "import torch\n",
        "\n",
        "print(\"✅ torch 버전:\", torch.__version__)\n",
        "print(\"✅ MPS 사용 가능 여부:\", torch.backends.mps.is_available())\n",
        "print(\"✅ MPS 사용 가능(빌드 포함) 여부:\", torch.backends.mps.is_built())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06f4cbb7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 2. Tensor 생성 방법\n",
        "# =============================================================================\n",
        "# 학습 목표: 다양한 Tensor 생성 API를 익힌다\n",
        "# Java 비유: 배열을 다양한 방식으로 초기화하는 것과 유사\n",
        "\n",
        "# 1) 직접 생성\n",
        "tensor_a = torch.tensor([1, 2, 3])\n",
        "print(\"tensor_a:\", tensor_a)\n",
        "\n",
        "# 2) 0으로 초기화\n",
        "tensor_b = torch.zeros(3, 3)\n",
        "print(\"tensor_b:\\n\", tensor_b)\n",
        "\n",
        "# 3) 1로 초기화\n",
        "tensor_c = torch.ones(2, 4)\n",
        "print(\"tensor_c:\\n\", tensor_c)\n",
        "\n",
        "# 4) 정규분포 랜덤 생성\n",
        "tensor_d = torch.randn(2, 3)\n",
        "print(\"tensor_d:\\n\", tensor_d)\n",
        "\n",
        "# 5) 범위 생성\n",
        "tensor_e = torch.arange(0, 10, 2)\n",
        "print(\"tensor_e:\", tensor_e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72135e88",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 3. NumPy와 상호 변환 (메모리 공유 주의)\n",
        "# =============================================================================\n",
        "# 학습 목표: numpy ↔ tensor 변환과 메모리 공유 특성을 이해한다\n",
        "# Java 비유: 배열 참조를 공유하면 한쪽 변경이 반영되는 상황과 유사\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "np_array = np.array([10, 20, 30], dtype=np.float32)\n",
        "torch_tensor = torch.from_numpy(np_array)\n",
        "print(\"numpy -> tensor:\", torch_tensor)\n",
        "\n",
        "# tensor -> numpy\n",
        "back_to_numpy = torch_tensor.numpy()\n",
        "print(\"tensor -> numpy:\", back_to_numpy)\n",
        "\n",
        "# 메모리 공유 확인 (tensor 변경 시 numpy도 변경됨)\n",
        "torch_tensor[0] = 999\n",
        "print(\"변경 후 numpy:\", np_array)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2909c968",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 4. Tensor 기본 연산\n",
        "# =============================================================================\n",
        "# 학습 목표: 산술 연산, 행렬 곱셈, 전치, reshape/view 차이를 이해한다\n",
        "# Java 비유: 루프 기반 연산을 벡터화해 속도를 높이는 것과 유사\n",
        "\n",
        "x = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
        "y = torch.tensor([[5.0, 6.0], [7.0, 8.0]])\n",
        "\n",
        "# 기본 산술 연산\n",
        "print(\"x + y:\\n\", x + y)\n",
        "print(\"x - y:\\n\", x - y)\n",
        "print(\"x * y:\\n\", x * y)  # 원소별 곱\n",
        "print(\"x / y:\\n\", x / y)\n",
        "\n",
        "# 행렬 곱셈 (matmul 또는 @)\n",
        "print(\"x @ y:\\n\", x @ y)\n",
        "print(\"torch.matmul(x, y):\\n\", torch.matmul(x, y))\n",
        "\n",
        "# 전치\n",
        "print(\"x.T:\\n\", x.T)\n",
        "\n",
        "# reshape vs view\n",
        "z = torch.arange(0, 6)\n",
        "reshape_z = z.reshape(2, 3)\n",
        "view_z = z.view(2, 3)\n",
        "print(\"reshape 결과:\\n\", reshape_z)\n",
        "print(\"view 결과:\\n\", view_z)\n",
        "\n",
        "# 차이 설명 (주석)\n",
        "# - reshape: 필요 시 복사 발생 가능 (안전)\n",
        "# - view: 메모리를 공유하는 뷰 생성 (연속 메모리 필요)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70f328c8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 5. GPU(MPS) 사용 및 속도 비교\n",
        "# =============================================================================\n",
        "# 학습 목표: CPU vs MPS 성능 차이를 확인한다\n",
        "# Java 비유: CPU 연산 vs GPU 가속 연산의 처리량 차이 비교\n",
        "\n",
        "import time\n",
        "\n",
        "# MPS 사용 가능 여부 확인\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "    print(\"✅ MPS 디바이스 사용\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"⚠️ MPS 사용 불가, CPU로 진행\")\n",
        "\n",
        "# CPU 텐서 생성\n",
        "cpu_tensor = torch.randn(2000, 2000)\n",
        "\n",
        "# MPS로 전송\n",
        "mps_tensor = cpu_tensor.to(device)\n",
        "\n",
        "# 연산 속도 비교 함수\n",
        "def benchmark_matmul(tensor, name: str) -> None:\n",
        "    start = time.time()\n",
        "    result = tensor @ tensor\n",
        "    # 연산 완료 동기화\n",
        "    if tensor.device.type == \"mps\":\n",
        "        torch.mps.synchronize()\n",
        "    end = time.time()\n",
        "    print(f\"{name} 연산 시간: {end - start:.4f}초\")\n",
        "\n",
        "# CPU 연산\n",
        "benchmark_matmul(cpu_tensor, \"CPU\")\n",
        "\n",
        "# MPS 연산 (가능한 경우)\n",
        "if device.type == \"mps\":\n",
        "    benchmark_matmul(mps_tensor, \"MPS\")\n",
        "\n",
        "# 결과 확인용 출력\n",
        "print(\"✅ 연산 완료\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "227c91f7",
      "metadata": {},
      "source": [
        "## 자동 미분(Autograd) 기초\n",
        "\n",
        "### 학습 포인트\n",
        "- `requires_grad=True`로 계산 그래프가 생성된다.\n",
        "- `backward()` 호출 시 미분값(gradient)이 누적된다.\n",
        "- Autograd는 **역전파(Backpropagation)**의 핵심 메커니즘이다.\n",
        "\n",
        "### Java 개발자 관점\n",
        "- Autograd는 “수식 트리”를 만들고, 그 트리를 따라 **자동 미분**을 수행하는 시스템이다.\n",
        "- 수동 미분 코드를 작성할 필요 없이, **계산 그래프 기반**으로 미분값이 전파된다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfde5c0a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 6. requires_grad 이해 및 계산 그래프\n",
        "# =============================================================================\n",
        "# 학습 목표: requires_grad로 미분 가능한 텐서를 만들고 그래프를 확인한다\n",
        "# Java 비유: 실행 흐름(연산)을 기록해 나중에 역방향 계산하는 것과 유사\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = torch.tensor([2.0], requires_grad=True)\n",
        "\n",
        "y = x**2 + 2 * x + 1  # y = x^2 + 2x + 1\n",
        "print(\"y 값:\", y.item())\n",
        "\n",
        "# 계산 그래프 정보 출력\n",
        "print(\"y.grad_fn:\", y.grad_fn)\n",
        "\n",
        "# 시각화: 함수 곡선과 x=2 지점 표시\n",
        "x_vals = torch.linspace(-5, 5, steps=100)\n",
        "y_vals = x_vals**2 + 2 * x_vals + 1\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(x_vals.numpy(), y_vals.numpy(), label=\"y = x^2 + 2x + 1\")\n",
        "plt.scatter([x.item()], [y.item()], color=\"red\", label=\"x=2\")\n",
        "plt.title(\"함수 그래프와 계산 지점\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f48b0599",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 7. 간단한 함수 미분 (y = x^2 + 2x + 1)\n",
        "# =============================================================================\n",
        "# 학습 목표: backward()로 미분값을 얻고 수학적 계산과 비교한다\n",
        "# Java 비유: 수식의 도함수를 자동으로 계산해 값을 얻는 것과 유사\n",
        "\n",
        "# 미분 계산\n",
        "x = torch.tensor([2.0], requires_grad=True)\n",
        "y = x**2 + 2 * x + 1\n",
        "\n",
        "y.backward()  # dy/dx 계산\n",
        "print(\"x.grad (자동 미분):\", x.grad.item())\n",
        "\n",
        "# 수학적 계산: dy/dx = 2x + 2\n",
        "manual_grad = 2 * x.item() + 2\n",
        "print(\"수학적 미분값:\", manual_grad)\n",
        "\n",
        "# 시각화: 접선의 기울기 표현\n",
        "x_vals = torch.linspace(-5, 5, steps=100)\n",
        "y_vals = x_vals**2 + 2 * x_vals + 1\n",
        "\n",
        "# 접선 방정식: y = f(a) + f'(a)(x - a)\n",
        "a = 2.0\n",
        "fa = a**2 + 2 * a + 1\n",
        "slope = manual_grad\n",
        "\n",
        "line_vals = slope * (x_vals - a) + fa\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(x_vals.numpy(), y_vals.numpy(), label=\"y = x^2 + 2x + 1\")\n",
        "plt.plot(x_vals.numpy(), line_vals.numpy(), linestyle=\"--\", label=\"접선\")\n",
        "plt.scatter([a], [fa], color=\"red\")\n",
        "plt.title(\"미분값(기울기) 시각화\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3481903d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 8. 다변수 함수 미분 (편미분)\n",
        "# =============================================================================\n",
        "# 학습 목표: 편미분 개념과 ∂z/∂x, ∂z/∂y 계산을 이해한다\n",
        "# Java 비유: 여러 입력 변수에 대한 영향도를 각각 계산하는 것과 유사\n",
        "\n",
        "x = torch.tensor([2.0], requires_grad=True)\n",
        "y = torch.tensor([3.0], requires_grad=True)\n",
        "\n",
        "z = x**2 + y**3  # z = x^2 + y^3\n",
        "z.backward()  # 스칼라이므로 바로 backward 가능\n",
        "\n",
        "print(\"∂z/∂x (자동 미분):\", x.grad.item())\n",
        "print(\"∂z/∂y (자동 미분):\", y.grad.item())\n",
        "\n",
        "# 수학적 계산\n",
        "manual_dx = 2 * x.item()  # dz/dx = 2x\n",
        "manual_dy = 3 * (y.item() ** 2)  # dz/dy = 3y^2\n",
        "\n",
        "print(\"∂z/∂x (수학적):\", manual_dx)\n",
        "print(\"∂z/∂y (수학적):\", manual_dy)\n",
        "\n",
        "# 시각화: z = x^2 + y^3 (contour)\n",
        "xs = torch.linspace(-3, 3, steps=50)\n",
        "ys = torch.linspace(-3, 3, steps=50)\n",
        "X, Y = torch.meshgrid(xs, ys, indexing=\"xy\")\n",
        "Z = X**2 + Y**3\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "contour = plt.contourf(X.numpy(), Y.numpy(), Z.numpy(), levels=30, cmap=\"viridis\")\n",
        "plt.colorbar(contour)\n",
        "plt.scatter([x.item()], [y.item()], color=\"red\", label=\"(x=2, y=3)\")\n",
        "plt.title(\"다변수 함수 z = x^2 + y^3 (Contour)\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7307c9f4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 9. 체인룰 (Chain Rule) 이해\n",
        "# =============================================================================\n",
        "# 학습 목표: 합성 함수에서 미분이 어떻게 전파되는지 이해한다\n",
        "# Java 비유: 여러 단계 계산의 영향을 거슬러 올라가 계산하는 것과 유사\n",
        "\n",
        "x = torch.tensor([1.5], requires_grad=True)\n",
        "\n",
        "# 합성 함수: y = (3x + 1)^2\n",
        "u = 3 * x + 1\n",
        "w = u**2\n",
        "\n",
        "w.backward()\n",
        "print(\"자동 미분 결과 dw/dx:\", x.grad.item())\n",
        "\n",
        "# 수학적 계산\n",
        "# w = (3x + 1)^2 -> dw/dx = 2(3x+1) * 3\n",
        "manual_grad = 2 * (3 * x.item() + 1) * 3\n",
        "print(\"수학적 미분값:\", manual_grad)\n",
        "\n",
        "# 시각화: 합성 함수 그래프\n",
        "x_vals = torch.linspace(-3, 3, steps=100)\n",
        "w_vals = (3 * x_vals + 1) ** 2\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(x_vals.numpy(), w_vals.numpy(), label=\"w = (3x + 1)^2\")\n",
        "plt.scatter([x.item()], [w.item()], color=\"red\", label=\"x=1.5\")\n",
        "plt.title(\"체인룰 적용된 합성 함수\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"w\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8688cf53",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 10. 경사하강법 시뮬레이션\n",
        "# =============================================================================\n",
        "# 학습 목표: f(x) = (x-3)^2 최소값을 경사하강법으로 찾는다\n",
        "# Java 비유: 반복문으로 최적값을 점진적으로 찾는 방식과 유사\n",
        "\n",
        "# 목적 함수\n",
        "\n",
        "def f(x_val: torch.Tensor) -> torch.Tensor:\n",
        "    return (x_val - 3) ** 2\n",
        "\n",
        "# 학습률 비교\n",
        "learning_rates = [0.1, 0.3]\n",
        "iterations = 20\n",
        "\n",
        "plt.figure(figsize=(7, 4))\n",
        "\n",
        "for lr in learning_rates:\n",
        "    x = torch.tensor([0.0], requires_grad=True)\n",
        "    history = []\n",
        "\n",
        "    for _ in range(iterations):\n",
        "        y = f(x)\n",
        "        y.backward()\n",
        "\n",
        "        # 경사하강법 업데이트\n",
        "        with torch.no_grad():\n",
        "            x -= lr * x.grad\n",
        "\n",
        "        history.append((x.item(), y.item()))\n",
        "        x.grad.zero_()\n",
        "\n",
        "    # 경로 시각화\n",
        "    xs_plot = torch.linspace(-1, 6, steps=100)\n",
        "    ys_plot = f(xs_plot)\n",
        "\n",
        "    plt.plot(xs_plot.numpy(), ys_plot.numpy(), color=\"gray\", alpha=0.5)\n",
        "    plt.scatter(\n",
        "        [h[0] for h in history],\n",
        "        [h[1] for h in history],\n",
        "        label=f\"lr={lr}\",\n",
        "    )\n",
        "\n",
        "plt.title(\"경사하강법 수렴 경로\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"f(x)\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "035bb05b",
      "metadata": {},
      "source": [
        "## 선형 회귀를 처음부터 구현 (핵심 학습 루프)\n",
        "\n",
        "### 학습 포인트\n",
        "- **직접 학습 루프 작성**: 예측 → 손실 → 역전파 → 업데이트의 기본 흐름\n",
        "- **backward() / grad / zero_() 역할**\n",
        "  - `backward()`는 미분값 계산\n",
        "  - `grad`는 기울기 저장\n",
        "  - `zero_()`는 기울기 초기화(누적 방지)\n",
        "- **학습률(learning rate)**은 수렴 속도와 안정성에 결정적\n",
        "\n",
        "> 이 구조가 모든 딥러닝 학습의 기본 원리입니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0bb1ff6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 11. 데이터 생성 (y = 2x + 3 + noise)\n",
        "# =============================================================================\n",
        "# 학습 목표: 선형 회귀용 가짜 데이터를 만든다\n",
        "# Java 비유: 테스트 데이터셋을 생성해 로직을 검증하는 단계와 유사\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "# 재현성을 위한 시드 고정\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# x: 0~10 범위의 100개 샘플\n",
        "x_np = np.linspace(0, 10, 100)\n",
        "noise = np.random.normal(0, 1, size=x_np.shape)\n",
        "y_np = 2 * x_np + 3 + noise\n",
        "\n",
        "# Tensor 변환\n",
        "x = torch.tensor(x_np, dtype=torch.float32).view(-1, 1)\n",
        "y = torch.tensor(y_np, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# 산점도 시각화\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.scatter(x_np, y_np, alpha=0.7)\n",
        "plt.title(\"선형 회귀 데이터 (y = 2x + 3 + noise)\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1191d00",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 12. 모델 정의 및 손실 함수 (MSE)\n",
        "# =============================================================================\n",
        "# 학습 목표: w, b를 학습 가능한 파라미터로 정의한다\n",
        "# Java 비유: 객체의 상태(w, b)를 업데이트하며 최적화하는 방식과 유사\n",
        "\n",
        "# 학습 가능한 파라미터 초기화\n",
        "w = torch.tensor([[0.0]], requires_grad=True)\n",
        "b = torch.tensor([[0.0]], requires_grad=True)\n",
        "\n",
        "# 예측 함수\n",
        "def predict(x_tensor: torch.Tensor) -> torch.Tensor:\n",
        "    return w * x_tensor + b\n",
        "\n",
        "# 손실 함수 (MSE)\n",
        "def mse_loss(y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
        "    return ((y_pred - y_true) ** 2).mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d3dc432",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 13. 경사하강법 직접 구현\n",
        "# =============================================================================\n",
        "# 학습 목표: backward(), grad, zero_() 역할을 직접 확인한다\n",
        "# Java 비유: 반복문으로 파라미터를 갱신하며 최적값을 찾는 방식과 유사\n",
        "\n",
        "learning_rate = 0.01\n",
        "epochs = 100\n",
        "\n",
        "loss_history = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # 예측\n",
        "    y_pred = predict(x)\n",
        "\n",
        "    # 손실 계산\n",
        "    loss = mse_loss(y_pred, y)\n",
        "\n",
        "    # 미분 계산\n",
        "    loss.backward()\n",
        "\n",
        "    # 파라미터 업데이트\n",
        "    with torch.no_grad():\n",
        "        w -= learning_rate * w.grad\n",
        "        b -= learning_rate * b.grad\n",
        "\n",
        "    # 그래디언트 초기화 (누적 방지)\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "\n",
        "    loss_history.append(loss.item())\n",
        "\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "        print(f\"Epoch {epoch+1}: loss={loss.item():.4f}, w={w.item():.4f}, b={b.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4911a59",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 14. 학습 과정 시각화 및 결과 비교\n",
        "# =============================================================================\n",
        "# 학습 목표: 학습 곡선과 최종 회귀선을 시각적으로 확인한다\n",
        "# Java 비유: 로그/그래프로 학습 상태를 모니터링하는 것과 유사\n",
        "\n",
        "# 1) epoch별 loss 그래프\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(loss_history, label=\"Loss\")\n",
        "plt.title(\"학습 손실 변화\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"MSE Loss\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2) 최종 학습된 직선과 원본 데이터 비교\n",
        "with torch.no_grad():\n",
        "    y_pred_final = predict(x)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.scatter(x_np, y_np, alpha=0.7, label=\"데이터\")\n",
        "plt.plot(x_np, y_pred_final.numpy(), color=\"red\", label=\"학습된 직선\")\n",
        "plt.title(\"학습 결과 비교\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3) 실제값(2,3)과 학습값 비교\n",
        "print(\"✅ 실제 파라미터: w=2, b=3\")\n",
        "print(f\"✅ 학습된 파라미터: w={w.item():.4f}, b={b.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0193109",
      "metadata": {},
      "source": [
        "## nn.Module 기반 신경망 (PyTorch 표준 구조)\n",
        "\n",
        "### Java 개발자 비유 (Spring Boot 관점)\n",
        "- `nn.Module` ≈ Java의 **abstract class** (공통 동작을 정의하고 확장)\n",
        "- `__init__` ≈ **생성자** (의존성/레이어 주입)\n",
        "- `forward()` ≈ **predict() 메서드** (요청 처리 로직)\n",
        "- Spring Boot의 컴포넌트처럼 **레이어를 조립**해서 하나의 모델 객체로 관리\n",
        "\n",
        "> 이 템플릿이 모든 PyTorch 모델의 기본입니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d5a9f4f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 15. nn.Module로 신경망 정의\n",
        "# =============================================================================\n",
        "# 학습 목표: nn.Module을 상속해 모델 구조를 정의한다\n",
        "# Java 비유: abstract class를 상속해 구현체를 만드는 것과 유사\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 레이어 정의: 10 -> 20 -> 1\n",
        "        self.fc1 = nn.Linear(10, 20)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(20, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 순전파 정의\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "model = SimpleNet()\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8009bb3e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 16. Loss와 Optimizer 비교 (SGD vs Adam)\n",
        "# =============================================================================\n",
        "# 학습 목표: 손실 함수와 최적화 알고리즘의 차이를 이해한다\n",
        "# Java 비유: 전략 패턴으로 최적화 로직을 교체하는 것과 유사\n",
        "\n",
        "# 더미 데이터 생성 (입력: 10차원, 타겟: 1차원)\n",
        "torch.manual_seed(42)\n",
        "inputs = torch.randn(200, 10)\n",
        "true_w = torch.randn(10, 1)\n",
        "true_b = torch.randn(1)\n",
        "targets = inputs @ true_w + true_b + 0.1 * torch.randn(200, 1)\n",
        "\n",
        "# 손실 함수\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# 옵티마이저 생성 (비교용)\n",
        "model_sgd = SimpleNet()\n",
        "optimizer_sgd = torch.optim.SGD(model_sgd.parameters(), lr=0.01)\n",
        "\n",
        "model_adam = SimpleNet()\n",
        "optimizer_adam = torch.optim.Adam(model_adam.parameters(), lr=0.01)\n",
        "\n",
        "print(\"✅ Optimizer 준비 완료: SGD, Adam\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "379e6c52",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 17. 학습 루프 템플릿\n",
        "# =============================================================================\n",
        "# 학습 목표: forward -> loss -> backward -> update 흐름을 이해한다\n",
        "# Java 비유: 요청 처리 흐름(컨트롤러→서비스→리포지토리)을 반복하는 것과 유사\n",
        "\n",
        "epochs = 50\n",
        "\n",
        "# SGD 학습\n",
        "loss_history_sgd = []\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    outputs = model_sgd(inputs)\n",
        "    loss = criterion(outputs, targets)\n",
        "\n",
        "    # Backward pass\n",
        "    optimizer_sgd.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer_sgd.step()\n",
        "\n",
        "    loss_history_sgd.append(loss.item())\n",
        "\n",
        "    # 로깅\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"[SGD] Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Adam 학습\n",
        "loss_history_adam = []\n",
        "for epoch in range(epochs):\n",
        "    outputs = model_adam(inputs)\n",
        "    loss = criterion(outputs, targets)\n",
        "\n",
        "    optimizer_adam.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer_adam.step()\n",
        "\n",
        "    loss_history_adam.append(loss.item())\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"[Adam] Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# 학습 곡선 비교\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(loss_history_sgd, label=\"SGD\")\n",
        "plt.plot(loss_history_adam, label=\"Adam\")\n",
        "plt.title(\"Optimizer 비교 (Loss 곡선)\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc22a208",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 18. 모델 저장/로드\n",
        "# =============================================================================\n",
        "# 학습 목표: 학습된 가중치를 저장하고 복원한다\n",
        "# Java 비유: 객체를 직렬화해서 저장/복원하는 것과 유사\n",
        "\n",
        "# 모델 저장\n",
        "model_path = \"model.pth\"\n",
        "torch.save(model_adam.state_dict(), model_path)\n",
        "print(f\"✅ 모델 저장 완료: {model_path}\")\n",
        "\n",
        "# 모델 로드\n",
        "loaded_model = SimpleNet()\n",
        "loaded_model.load_state_dict(torch.load(model_path))\n",
        "loaded_model.eval()  # 추론 모드\n",
        "print(\"✅ 모델 로드 완료\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb8648cc",
      "metadata": {},
      "source": [
        "## MNIST 데이터셋 로딩 및 탐색\n",
        "\n",
        "### 학습 포인트\n",
        "- **DataLoader 역할**: 배치 처리 + 셔플링으로 학습 효율 향상\n",
        "- **transform 중요성**: 정규화로 학습 안정성 확보\n",
        "- **Epoch / Batch / Iteration**\n",
        "  - Epoch: 전체 데이터 1회 학습\n",
        "  - Batch: 한번에 처리하는 샘플 묶음\n",
        "  - Iteration: 배치 1회 처리\n",
        "\n",
        "### Java 개발자 관점\n",
        "- Dataset ≈ Collection\n",
        "- DataLoader ≈ Iterator + 배치 처리\n",
        "- batch_size는 **메모리 사용량**과 직결\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75f8532c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 19. MNIST 데이터셋 다운로드 및 DataLoader 생성\n",
        "# =============================================================================\n",
        "# 학습 목표: torchvision으로 MNIST를 로드하고 배치 단위로 처리한다\n",
        "# Java 비유: Collection을 Iterator로 순회하며 배치 처리하는 방식과 유사\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# transform 설정 (정규화 포함)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,)),  # MNIST 평균/표준편차\n",
        "])\n",
        "\n",
        "# MNIST 데이터셋 로드\n",
        "train_dataset = torchvision.datasets.MNIST(\n",
        "    root=\"./data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform,\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(\n",
        "    root=\"./data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=transform,\n",
        ")\n",
        "\n",
        "# DataLoader 생성\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        ")\n",
        "\n",
        "print(\"✅ MNIST 데이터셋 로드 완료\")\n",
        "print(\"- Train size:\", len(train_dataset))\n",
        "print(\"- Test size:\", len(test_dataset))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0801a0c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 20. 데이터 배치 확인 및 샘플 시각화\n",
        "# =============================================================================\n",
        "# 학습 목표: 배치 크기와 이미지 형태를 확인하고 데이터 분포를 시각적으로 이해한다\n",
        "# Java 비유: Iterator.next()로 배치를 가져와 구조를 확인하는 것과 유사\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 배치 1개 가져오기\n",
        "images, labels = next(iter(train_loader))\n",
        "\n",
        "print(\"✅ 배치 크기:\", images.size(0))\n",
        "print(\"✅ 이미지 shape:\", images.shape)  # (batch_size, 1, 28, 28)\n",
        "\n",
        "# 5x5 그리드 시각화\n",
        "plt.figure(figsize=(6, 6))\n",
        "for i in range(25):\n",
        "    plt.subplot(5, 5, i + 1)\n",
        "    plt.imshow(images[i][0], cmap=\"gray\")\n",
        "    plt.title(str(labels[i].item()))\n",
        "    plt.axis(\"off\")\n",
        "plt.suptitle(\"MNIST 샘플 (5x5)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 클래스별 샘플 1개씩 시각화\n",
        "class_samples = {}\n",
        "for img, label in train_dataset:\n",
        "    if label not in class_samples:\n",
        "        class_samples[label] = img\n",
        "    if len(class_samples) == 10:\n",
        "        break\n",
        "\n",
        "plt.figure(figsize=(8, 3))\n",
        "for i in range(10):\n",
        "    plt.subplot(2, 5, i + 1)\n",
        "    plt.imshow(class_samples[i][0], cmap=\"gray\")\n",
        "    plt.title(f\"Class {i}\")\n",
        "    plt.axis(\"off\")\n",
        "plt.suptitle(\"클래스별 샘플 1개\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7966c136",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 21. 데이터 통계 확인 (클래스 분포)\n",
        "# =============================================================================\n",
        "# 학습 목표: 클래스별 샘플 수를 확인해 데이터 균형을 평가한다\n",
        "# Java 비유: 카운팅 집계(Count by Group)와 유사\n",
        "\n",
        "# 클래스별 샘플 개수\n",
        "class_counts = torch.bincount(train_dataset.targets)\n",
        "print(\"✅ 클래스별 샘플 개수:\")\n",
        "for i, count in enumerate(class_counts.tolist()):\n",
        "    print(f\"Class {i}: {count}\")\n",
        "\n",
        "# 클래스 분포 시각화\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar(range(10), class_counts.numpy())\n",
        "plt.title(\"MNIST 클래스 분포\")\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a440eaf",
      "metadata": {},
      "source": [
        "## CNN 모델 학습 (MNIST)\n",
        "\n",
        "### 실무 고려사항\n",
        "- **학습 시간**: 모델/배치 크기에 따라 크게 달라짐\n",
        "- **GPU 메모리**: 배치 크기를 조정해 OOM 방지\n",
        "- **모니터링**: 학습/검증 지표를 함께 확인해 과적합 감지\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a9b7015",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 22. CNN 모델 정의 (입출력 크기 주석 포함)\n",
        "# =============================================================================\n",
        "# 학습 목표: 기본 CNN 구조를 구현하고 텐서 크기 흐름을 이해한다\n",
        "# Java 비유: 레이어를 조립해 파이프라인을 구성하는 것과 유사\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 입력: (N, 1, 28, 28)\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)  # -> (N, 16, 28, 28)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)  # -> (N, 16, 14, 14)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)  # -> (N, 32, 14, 14)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)  # -> (N, 32, 7, 7)\n",
        "\n",
        "        self.flatten = nn.Flatten()  # -> (N, 32*7*7)\n",
        "        self.fc1 = nn.Linear(32 * 7 * 7, 128)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 10)  # 출력: 10 클래스\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.pool2(self.relu2(self.conv2(x)))\n",
        "        x = self.flatten(x)\n",
        "        x = self.relu3(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "cnn_model = CNN()\n",
        "print(cnn_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e988d88a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 23. MPS(GPU) 설정 및 데이터 분할\n",
        "# =============================================================================\n",
        "# 학습 목표: MPS 사용 가능 시 GPU로 연산을 이동한다\n",
        "# Java 비유: 연산 엔진을 CPU -> GPU로 교체하는 것과 유사\n",
        "\n",
        "import time\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "\n",
        "# 디바이스 설정\n",
        "if torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "    print(\"✅ MPS 사용\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"⚠️ MPS 미사용, CPU로 진행\")\n",
        "\n",
        "cnn_model = cnn_model.to(device)\n",
        "\n",
        "# Train/Validation 분할 (예: 90% / 10%)\n",
        "train_size = int(len(train_dataset) * 0.9)\n",
        "val_size = len(train_dataset) - train_size\n",
        "\n",
        "train_subset, val_subset = random_split(\n",
        "    train_dataset,\n",
        "    [train_size, val_size],\n",
        "    generator=torch.Generator().manual_seed(42),\n",
        ")\n",
        "\n",
        "train_loader_cnn = DataLoader(\n",
        "    train_subset,\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        ")\n",
        "\n",
        "val_loader_cnn = DataLoader(\n",
        "    val_subset,\n",
        "    batch_size=64,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        ")\n",
        "\n",
        "print(\"✅ Train/Validation 분할 완료\")\n",
        "print(\"- Train size:\", len(train_subset))\n",
        "print(\"- Val size:\", len(val_subset))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bfa7c14",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 24. 학습/검증 루프 구현\n",
        "# =============================================================================\n",
        "# 학습 목표: 학습 과정과 검증 정확도를 확인한다\n",
        "# Java 비유: 서비스 로직 수행 후 검증 테스트를 주기적으로 수행하는 것과 유사\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(cnn_model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 5\n",
        "train_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # -------------------------\n",
        "    # Training Loop\n",
        "    # -------------------------\n",
        "    cnn_model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for inputs, targets in train_loader_cnn:\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        outputs = cnn_model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader_cnn)\n",
        "    train_losses.append(avg_loss)\n",
        "\n",
        "    # -------------------------\n",
        "    # Validation Loop\n",
        "    # -------------------------\n",
        "    cnn_model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in val_loader_cnn:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            outputs = cnn_model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "\n",
        "    val_acc = correct / total\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"✅ 학습 완료 (소요 시간: {end_time - start_time:.2f}초)\")\n",
        "\n",
        "# MPS 메모리 사용량 (가능한 경우)\n",
        "if device.type == \"mps\" and hasattr(torch.mps, \"current_allocated_memory\"):\n",
        "    print(\"✅ MPS 메모리 사용량(byte):\", torch.mps.current_allocated_memory())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13b6e646",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 25. 학습 모니터링 시각화 (Loss / Val Accuracy)\n",
        "# =============================================================================\n",
        "# 학습 목표: 과적합 여부를 시각적으로 확인한다\n",
        "# Java 비유: 로그/모니터링 대시보드로 상태를 보는 것과 유사\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label=\"Train Loss\")\n",
        "plt.title(\"Train Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(val_accuracies, label=\"Val Accuracy\", color=\"orange\")\n",
        "plt.title(\"Validation Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "086aba5d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 26. 최종 평가 (Test Accuracy / Confusion Matrix / 클래스별 정확도)\n",
        "# =============================================================================\n",
        "# 학습 목표: 테스트 성능을 정량적으로 평가한다\n",
        "# Java 비유: 최종 통합 테스트 결과를 정리하는 단계와 유사\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "cnn_model.eval()\n",
        "all_preds = []\n",
        "all_targets = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        outputs = cnn_model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "# Test Accuracy\n",
        "all_preds_tensor = torch.tensor(all_preds)\n",
        "all_targets_tensor = torch.tensor(all_targets)\n",
        "\n",
        "test_acc = (all_preds_tensor == all_targets_tensor).float().mean().item()\n",
        "print(f\"✅ Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(all_targets, all_preds)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 클래스별 정확도\n",
        "class_correct = [0] * 10\n",
        "class_total = [0] * 10\n",
        "\n",
        "for pred, target in zip(all_preds, all_targets):\n",
        "    class_total[target] += 1\n",
        "    if pred == target:\n",
        "        class_correct[target] += 1\n",
        "\n",
        "print(\"✅ 클래스별 정확도\")\n",
        "for i in range(10):\n",
        "    acc = class_correct[i] / class_total[i] if class_total[i] > 0 else 0\n",
        "    print(f\"Class {i}: {acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f816da85",
      "metadata": {},
      "source": [
        "## 상세 모델 평가 및 오류 분석\n",
        "\n",
        "### 실무 인사이트\n",
        "- 프로덕션 배포 전 **오류 패턴**과 **신뢰도**를 반드시 점검해야 한다.\n",
        "- 단순 정확도 외에 **Precision/Recall/F1**을 함께 확인해 리스크를 낮춘다.\n",
        "\n",
        "### 금융권 AI 검증 절차 비교\n",
        "- 신용평가 모델과 동일하게 **오류 사례 리뷰**와 **클래스별 성능 검증**이 필수\n",
        "- 규제/감사 대응을 위해 **설명 가능한 오류 분석 로그**를 남겨야 한다\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a327789",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 27. 예측 결과 분석 (Accuracy/Precision/Recall/F1)\n",
        "# =============================================================================\n",
        "# 학습 목표: 분류 모델의 핵심 지표를 종합적으로 확인한다\n",
        "# Java 비유: 단순 성공률 외에 세부 지표를 함께 보는 품질 검증과 유사\n",
        "\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
        "\n",
        "# 전체 테스트 셋 예측 및 확률\n",
        "cnn_model.eval()\n",
        "all_logits = []\n",
        "all_preds = []\n",
        "all_targets = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        logits = cnn_model(inputs)\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        _, predicted = torch.max(probs, 1)\n",
        "\n",
        "        all_logits.append(probs.cpu())\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_targets.extend(targets.cpu().numpy())\n",
        "\n",
        "all_probs = torch.cat(all_logits, dim=0)\n",
        "\n",
        "# classification report\n",
        "print(\"✅ Classification Report\")\n",
        "print(classification_report(all_targets, all_preds))\n",
        "\n",
        "# 전체 Precision/Recall/F1 요약\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "    all_targets, all_preds, average=\"macro\"\n",
        ")\n",
        "print(f\"✅ Macro Precision: {precision:.4f}\")\n",
        "print(f\"✅ Macro Recall: {recall:.4f}\")\n",
        "print(f\"✅ Macro F1-Score: {f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "361ea553",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 28. Confusion Matrix 및 혼동 패턴 분석\n",
        "# =============================================================================\n",
        "# 학습 목표: 어떤 숫자를 자주 헷갈리는지 확인한다\n",
        "# Java 비유: 에러 케이스를 카테고리별로 집계하는 것과 유사\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "cm = confusion_matrix(all_targets, all_preds)\n",
        "\n",
        "plt.figure(figsize=(7, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix (0~9)\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 혼동 패턴 분석: 대각선 제외 상위 오분류\n",
        "cm_no_diag = cm.copy()\n",
        "np.fill_diagonal(cm_no_diag, 0)\n",
        "\n",
        "confusions = []\n",
        "for i in range(10):\n",
        "    for j in range(10):\n",
        "        if cm_no_diag[i, j] > 0:\n",
        "            confusions.append((i, j, cm_no_diag[i, j]))\n",
        "\n",
        "confusions_sorted = sorted(confusions, key=lambda x: x[2], reverse=True)\n",
        "print(\"✅ 자주 혼동되는 숫자 Top 5\")\n",
        "for actual, pred, count in confusions_sorted[:5]:\n",
        "    print(f\"Actual {actual} -> Pred {pred}: {count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e43ede7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 29. 오분류 사례 분석 (Top 20)\n",
        "# =============================================================================\n",
        "# 학습 목표: 모델이 어떤 이미지를 틀리는지 시각적으로 확인한다\n",
        "# Java 비유: 실패 케이스를 샘플링해 디버깅하는 방식과 유사\n",
        "\n",
        "# 오분류 인덱스 추출\n",
        "mis_idx = [i for i, (p, t) in enumerate(zip(all_preds, all_targets)) if p != t]\n",
        "\n",
        "# Top 20 오분류 샘플\n",
        "top_n = 20\n",
        "sample_idx = mis_idx[:top_n]\n",
        "\n",
        "# 테스트 데이터에서 원본 이미지 가져오기\n",
        "plt.figure(figsize=(10, 8))\n",
        "for i, idx in enumerate(sample_idx):\n",
        "    img, label = test_dataset[idx]\n",
        "    pred = all_preds[idx]\n",
        "\n",
        "    plt.subplot(4, 5, i + 1)\n",
        "    plt.imshow(img[0], cmap=\"gray\")\n",
        "    plt.title(f\"Pred:{pred} / True:{label}\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.suptitle(\"오분류 Top 20 사례\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c83278bd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 30. 클래스별 성능 분석\n",
        "# =============================================================================\n",
        "# 학습 목표: 각 숫자별 정확도를 비교한다\n",
        "# Java 비유: 기능별 성공률을 비교해 취약 영역을 찾는 것과 유사\n",
        "\n",
        "class_correct = [0] * 10\n",
        "class_total = [0] * 10\n",
        "\n",
        "for pred, target in zip(all_preds, all_targets):\n",
        "    class_total[target] += 1\n",
        "    if pred == target:\n",
        "        class_correct[target] += 1\n",
        "\n",
        "class_acc = [\n",
        "    class_correct[i] / class_total[i] if class_total[i] > 0 else 0\n",
        "    for i in range(10)\n",
        "]\n",
        "\n",
        "best_class = int(np.argmax(class_acc))\n",
        "worst_class = int(np.argmin(class_acc))\n",
        "\n",
        "print(\"✅ 클래스별 정확도\")\n",
        "for i, acc in enumerate(class_acc):\n",
        "    print(f\"Class {i}: {acc:.4f}\")\n",
        "\n",
        "print(f\"✅ 가장 잘 맞추는 클래스: {best_class}\")\n",
        "print(f\"✅ 가장 어려운 클래스: {worst_class}\")\n",
        "\n",
        "# 막대 그래프\n",
        "plt.figure(figsize=(7, 4))\n",
        "plt.bar(range(10), class_acc)\n",
        "plt.title(\"클래스별 정확도\")\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70c5fd96",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# 31. 신뢰도(Confidence) 분석\n",
        "# =============================================================================\n",
        "# 학습 목표: 확률 기반으로 모델 신뢰도를 점검한다\n",
        "# Java 비유: 결과에 대한 확신도를 함께 확인하는 로깅과 유사\n",
        "\n",
        "# softmax 확률 최대값 추출\n",
        "max_probs, pred_classes = torch.max(all_probs, dim=1)\n",
        "\n",
        "# 높은 확률로 틀린 케이스\n",
        "high_conf_wrong = [\n",
        "    i for i, (p, t, prob) in enumerate(zip(all_preds, all_targets, max_probs))\n",
        "    if p != t and prob.item() >= 0.9\n",
        "]\n",
        "\n",
        "# 낮은 확률로 맞춘 케이스\n",
        "low_conf_right = [\n",
        "    i for i, (p, t, prob) in enumerate(zip(all_preds, all_targets, max_probs))\n",
        "    if p == t and prob.item() <= 0.5\n",
        "]\n",
        "\n",
        "print(\"✅ 높은 확률로 틀린 케이스 수:\", len(high_conf_wrong))\n",
        "print(\"✅ 낮은 확률로 맞춘 케이스 수:\", len(low_conf_right))\n",
        "\n",
        "# 일부 사례 시각화 (최대 10개)\n",
        "show_n = 10\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "for i, idx in enumerate(high_conf_wrong[:show_n]):\n",
        "    img, label = test_dataset[idx]\n",
        "    pred = all_preds[idx]\n",
        "    prob = max_probs[idx].item()\n",
        "\n",
        "    plt.subplot(2, 5, i + 1)\n",
        "    plt.imshow(img[0], cmap=\"gray\")\n",
        "    plt.title(f\"P:{pred} T:{label}\\nConf:{prob:.2f}\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.suptitle(\"높은 확률로 틀린 케이스\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "for i, idx in enumerate(low_conf_right[:show_n]):\n",
        "    img, label = test_dataset[idx]\n",
        "    pred = all_preds[idx]\n",
        "    prob = max_probs[idx].item()\n",
        "\n",
        "    plt.subplot(2, 5, i + 1)\n",
        "    plt.imshow(img[0], cmap=\"gray\")\n",
        "    plt.title(f\"P:{pred} T:{label}\\nConf:{prob:.2f}\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.suptitle(\"낮은 확률로 맞춘 케이스\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
