# Week 2: PyTorch ì‹¬í™” + BERT (56ì‹œê°„)

> **ëª©í‘œ:** CNN, LSTM, Transformers, BERT Fine-tuning ë§ˆìŠ¤í„°

## ğŸ“… ì£¼ì°¨ ì¼ì •

### Day 8-10 (ì›”-ìˆ˜): CNN + RNN/LSTM ë§ˆìŠ¤í„°
**í•™ìŠµ ì‹œê°„:** 24ì‹œê°„
- Transfer Learning (ResNet)
- LSTM ì‹œê³„ì—´ ì˜ˆì¸¡
- Data Augmentation

### Day 11-14 (ëª©-ì¼): Transformers + BERT ì‹¤ì „
**í•™ìŠµ ì‹œê°„:** 32ì‹œê°„
- Hugging Face Transformers
- BERT Fine-tuning
- NLP íŒŒì´í”„ë¼ì¸

## ğŸ¯ í•™ìŠµ ëª©í‘œ

### í•µì‹¬ ì—­ëŸ‰
- âœ… CNN Transfer Learning (ResNet)
- âœ… LSTM ì‹œê³„ì—´ ì˜ˆì¸¡
- âœ… BERT Fine-tuning
- âœ… Hugging Face ë¼ì´ë¸ŒëŸ¬ë¦¬

### ì™„ì„± í”„ë¡œì íŠ¸
4. **ê¸ˆìœµ ë¬¸ì„œ ì´ë¯¸ì§€ ë¶„ë¥˜** - CNN
5. **ì£¼ê°€ ì˜ˆì¸¡ LSTM** - ì‹œê³„ì—´
6. **ê¸ˆìœµ ë‰´ìŠ¤ ê°ì„± ë¶„ì„** - BERT
7. **ê¸ˆìœµ ë¬¸ì„œ Q&A** - Extractive QA

## ğŸ“š í•™ìŠµ ê°•ì˜

### Day 8-10 (9ì‹œê°„ ê°•ì˜)
- Fast.ai Lesson 3-5 (CNN ì¤‘ì‹¬)
- Stanford CS231n (í•µì‹¬ ê°•ì˜ë§Œ)
- PyTorch Lightning íŠœí† ë¦¬ì–¼

### Day 11-14 (12ì‹œê°„ ê°•ì˜)
- Hugging Face Course (Chapter 1-4)
- "Attention is All You Need" ë…¼ë¬¸
- BERT êµ¬ì¡° ì´í•´

## ğŸ› ï¸ í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
conda activate ai-dev

# CNN & LSTM
conda install pytorch torchvision -c pytorch -y

# Transformers
pip install transformers datasets tokenizers
pip install accelerate
pip install sentencepiece

# í•œêµ­ì–´ NLP
pip install konlpy
```

## ğŸ’» ì‹¤ìŠµ í”„ë¡œì íŠ¸

### Project 4: ê¸ˆìœµ ë¬¸ì„œ ì´ë¯¸ì§€ ë¶„ë¥˜ (Day 8-10)

**íŒŒì¼:** `document_classifier.py`

```python
"""
ê¸ˆìœµ ë¬¸ì„œ ì´ë¯¸ì§€ ë¶„ë¥˜ (CNN Transfer Learning)
- ì‹ ë¶„ì¦, ê³„ì•½ì„œ, ì²­êµ¬ì„œ ë“± 5ê°œ í´ë˜ìŠ¤
- ResNet50 Transfer Learning
- Data Augmentation
- F1-Score 90% ì´ìƒ ëª©í‘œ
"""

import torch
import torch.nn as nn
from torchvision import models, transforms, datasets
from torch.utils.data import DataLoader

# 1. ë°ì´í„° ì „ì²˜ë¦¬ & Augmentation
train_transforms = transforms.Compose([
    transforms.RandomResizedCrop(224),
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

test_transforms = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

# 2. ëª¨ë¸ ì •ì˜ (ResNet50 Transfer Learning)
class FinancialDocClassifier(nn.Module):
    def __init__(self, num_classes=5):
        super().__init__()
        self.resnet = models.resnet50(pretrained=True)
        
        # Freeze early layers
        for param in list(self.resnet.parameters())[:-20]:
            param.requires_grad = False
        
        # Replace final layer
        num_features = self.resnet.fc.in_features
        self.resnet.fc = nn.Sequential(
            nn.Linear(num_features, 512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, num_classes)
        )
    
    def forward(self, x):
        return self.resnet(x)

# 3. í•™ìŠµ ë£¨í”„
def train_model(model, train_loader, val_loader, num_epochs=10):
    device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
    model = model.to(device)
    
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)
    
    best_acc = 0.0
    
    for epoch in range(num_epochs):
        # Training
        model.train()
        running_loss = 0.0
        
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            
            running_loss += loss.item()
        
        # Validation
        model.eval()
        correct = 0
        total = 0
        
        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                _, predicted = torch.max(outputs, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
        
        accuracy = 100 * correct / total
        print(f'Epoch {epoch+1}: Loss: {running_loss/len(train_loader):.4f}, Acc: {accuracy:.2f}%')
        
        if accuracy > best_acc:
            best_acc = accuracy
            torch.save(model.state_dict(), 'best_doc_classifier.pth')
        
        scheduler.step()
    
    return model

# 4. ì‹¤í–‰
model = FinancialDocClassifier(num_classes=5)
# train_model(model, train_loader, val_loader)
```

**ì²´í¬í¬ì¸íŠ¸:**
- [ ] Transfer Learning êµ¬í˜„
- [ ] Data Augmentation ì ìš©
- [ ] 90% ì´ìƒ ì •í™•ë„
- [ ] ëª¨ë¸ ì €ì¥/ë¡œë“œ

---

### Project 5: ì£¼ê°€ ì˜ˆì¸¡ LSTM (Day 8-10)

**íŒŒì¼:** `stock_lstm.py`

```python
"""
ì£¼ê°€ ì˜ˆì¸¡ LSTM
- ë‹¤ë³€ëŸ‰ ì‹œê³„ì—´ (ì‹œê°€, ê³ ê°€, ì €ê°€, ê±°ë˜ëŸ‰)
- 60ì¼ ë°ì´í„° â†’ ë‹¤ìŒë‚  ì˜ˆì¸¡
- RMSE, MAPE í‰ê°€
"""

import torch
import torch.nn as nn
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler

class StockLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.lstm = nn.LSTM(
            input_size, 
            hidden_size, 
            num_layers, 
            batch_first=True,
            dropout=0.2
        )
        self.fc = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        batch_size = x.size(0)
        
        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)
        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)
        
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :])
        return out

# ë°ì´í„° ì „ì²˜ë¦¬
def prepare_data(df, seq_length=60):
    scaler = MinMaxScaler()
    scaled_data = scaler.fit_transform(df)
    
    X, y = [], []
    for i in range(len(scaled_data) - seq_length):
        X.append(scaled_data[i:i+seq_length])
        y.append(scaled_data[i+seq_length, 3])  # Close price
    
    return np.array(X), np.array(y), scaler

# í•™ìŠµ
def train_lstm(model, train_loader, num_epochs=50):
    device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
    model = model.to(device)
    
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    
    for epoch in range(num_epochs):
        model.train()
        total_loss = 0
        
        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            
            optimizer.zero_grad()
            output = model(X_batch)
            loss = criterion(output, y_batch.unsqueeze(1))
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
        
        if (epoch+1) % 10 == 0:
            print(f'Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.6f}')
    
    return model

# í‰ê°€ (RMSE, MAPE)
def evaluate(model, test_loader, scaler):
    model.eval()
    predictions = []
    actuals = []
    
    with torch.no_grad():
        for X_batch, y_batch in test_loader:
            output = model(X_batch)
            predictions.extend(output.cpu().numpy())
            actuals.extend(y_batch.cpu().numpy())
    
    # Inverse transform
    predictions = scaler.inverse_transform(
        np.concatenate([np.zeros((len(predictions), 3)), 
                       np.array(predictions)], axis=1)
    )[:, 3]
    
    actuals = scaler.inverse_transform(
        np.concatenate([np.zeros((len(actuals), 3)), 
                       np.array(actuals).reshape(-1, 1)], axis=1)
    )[:, 3]
    
    rmse = np.sqrt(np.mean((predictions - actuals)**2))
    mape = np.mean(np.abs((actuals - predictions) / actuals)) * 100
    
    print(f'RMSE: {rmse:.2f}, MAPE: {mape:.2f}%')
```

**ì²´í¬í¬ì¸íŠ¸:**
- [ ] LSTM êµ¬ì¡° ì´í•´
- [ ] ì‹œê³„ì—´ ë°ì´í„° ì „ì²˜ë¦¬
- [ ] Walk-forward Validation
- [ ] RMSE < 5% ë‹¬ì„±

---

### Project 6: ê¸ˆìœµ ë‰´ìŠ¤ ê°ì„± ë¶„ì„ (Day 11-14)

**íŒŒì¼:** `financial_sentiment.py`

```python
"""
ê¸ˆìœµ ë‰´ìŠ¤ ê°ì„± ë¶„ì„ (BERT Fine-tuning)
- ëª¨ë¸: klue/bert-base
- 3-class: ê¸ì •/ì¤‘ë¦½/ë¶€ì •
- F1-Score 85% ì´ìƒ
"""

from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments,
    DataCollatorWithPadding
)
from datasets import load_dataset, Dataset
import torch

# 1. ë°ì´í„° ì¤€ë¹„
def prepare_dataset():
    # ê¸ˆìœµ ë‰´ìŠ¤ ë°ì´í„°ì…‹ (ì˜ˆì‹œ)
    data = {
        'text': [
            "ì£¼ê°€ê°€ ê¸‰ë“±í•˜ë©° íˆ¬ììë“¤ì˜ ê¸°ëŒ€ê°ì´ ë†’ì•„ì§€ê³  ìˆë‹¤",
            "ê²½ì œ ìœ„ê¸°ë¡œ ì¸í•œ ë¶ˆí™•ì‹¤ì„±ì´ ì§€ì†ë˜ê³  ìˆë‹¤",
            # ... more data
        ],
        'label': [2, 0, ...]  # 0: ë¶€ì •, 1: ì¤‘ë¦½, 2: ê¸ì •
    }
    
    dataset = Dataset.from_dict(data)
    return dataset.train_test_split(test_size=0.2)

# 2. í† í¬ë‚˜ì´ì € & ì „ì²˜ë¦¬
model_name = "klue/bert-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)

def preprocess_function(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        max_length=512,
        padding="max_length"
    )

# 3. ëª¨ë¸ ë¡œë“œ
model = AutoModelForSequenceClassification.from_pretrained(
    model_name, 
    num_labels=3
)

# 4. Training Arguments
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=64,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=100,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="f1",
)

# 5. í‰ê°€ ë©”íŠ¸ë¦­
from sklearn.metrics import f1_score, accuracy_score

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    
    acc = accuracy_score(labels, predictions)
    f1 = f1_score(labels, predictions, average='weighted')
    
    return {"accuracy": acc, "f1": f1}

# 6. Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_test,
    compute_metrics=compute_metrics,
)

# 7. í•™ìŠµ & í‰ê°€
trainer.train()
results = trainer.evaluate()
print(results)

# 8. ëª¨ë¸ ì €ì¥
model.save_pretrained("./financial_sentiment_model")
tokenizer.save_pretrained("./financial_sentiment_model")
```

**ì²´í¬í¬ì¸íŠ¸:**
- [ ] BERT Fine-tuning ì™„ë£Œ
- [ ] F1-Score 85% ì´ìƒ
- [ ] ëª¨ë¸ ì €ì¥/ë°°í¬ ì¤€ë¹„
- [ ] Inference íŒŒì´í”„ë¼ì¸

---

### Project 7: ê¸ˆìœµ ë¬¸ì„œ Q&A (Day 11-14)

**íŒŒì¼:** `document_qa.py`

```python
"""
ê¸ˆìœµ ë¬¸ì„œ Q&A (Extractive Question Answering)
- ê³„ì•½ì„œ, ì•½ê´€ì—ì„œ ì •ë³´ ì¶”ì¶œ
- ëª¨ë¸: klue/roberta-large
"""

from transformers import pipeline

# 1. QA íŒŒì´í”„ë¼ì¸
qa_pipeline = pipeline(
    "question-answering",
    model="klue/roberta-large",
    tokenizer="klue/roberta-large"
)

# 2. ê¸ˆìœµ ê³„ì•½ì„œ ì˜ˆì‹œ
context = """
ë³¸ ëŒ€ì¶œ ê³„ì•½ì˜ ì´ììœ¨ì€ ì—° 4.5%ì´ë©°, ìƒí™˜ ê¸°ê°„ì€ 36ê°œì›”ì…ë‹ˆë‹¤. 
ì¡°ê¸° ìƒí™˜ ì‹œ ìœ„ì•½ê¸ˆì€ ì—†ìœ¼ë‚˜, ìµœì†Œ 6ê°œì›” ì´í›„ë¶€í„° ê°€ëŠ¥í•©ë‹ˆë‹¤.
ëŒ€ì¶œ í•œë„ëŠ” ìµœëŒ€ 5ì²œë§Œì›ì´ë©°, ë‹´ë³´ëŠ” ë¶€ë™ì‚°ìœ¼ë¡œ ì„¤ì •ë©ë‹ˆë‹¤.
"""

# 3. ì§ˆë¬¸ & ë‹µë³€
questions = [
    "ì´ììœ¨ì€ ì–¼ë§ˆì¸ê°€ìš”?",
    "ìƒí™˜ ê¸°ê°„ì€?",
    "ì¡°ê¸° ìƒí™˜ ìœ„ì•½ê¸ˆì€?",
    "ëŒ€ì¶œ í•œë„ëŠ” ì–¼ë§ˆì¸ê°€ìš”?"
]

for question in questions:
    result = qa_pipeline(question=question, context=context)
    print(f"\nQ: {question}")
    print(f"A: {result['answer']}")
    print(f"Confidence: {result['score']:.4f}")

# 4. ë°°ì¹˜ ì²˜ë¦¬
def batch_qa(questions, context):
    results = []
    for q in questions:
        result = qa_pipeline(question=q, context=context)
        results.append({
            'question': q,
            'answer': result['answer'],
            'confidence': result['score']
        })
    return results
```

**ì²´í¬í¬ì¸íŠ¸:**
- [ ] Extractive QA ì´í•´
- [ ] ê¸ˆìœµ ë¬¸ì„œ ì²˜ë¦¬
- [ ] ë°°ì¹˜ ì²˜ë¦¬ êµ¬í˜„
- [ ] 90% ì´ìƒ ì •í™•ë„

---

## âœ… Week 2 ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸

### í”„ë¡œì íŠ¸ ì™„ì„±ë„
- [ ] Project 4: ë¬¸ì„œ ë¶„ë¥˜ 90%+ âœ…
- [ ] Project 5: LSTM ì˜ˆì¸¡ RMSE < 5% âœ…
- [ ] Project 6: BERT ê°ì„± ë¶„ì„ F1 85%+ âœ…
- [ ] Project 7: ë¬¸ì„œ Q&A 90%+ âœ…

### ê¸°ìˆ  ìŠµë“
- [ ] Transfer Learning ë§ˆìŠ¤í„°
- [ ] LSTM ì‹œê³„ì—´ ì˜ˆì¸¡
- [ ] BERT Fine-tuning
- [ ] Hugging Face í™œìš©

### GitHub
- [ ] 4ê°œ í”„ë¡œì íŠ¸ ì»¤ë°‹
- [ ] ëª¨ë¸ íŒŒì¼ ì €ì¥
- [ ] í•™ìŠµ ë¡œê·¸ ì •ë¦¬

### ë‹¤ìŒ ì£¼ ì¤€ë¹„
- [ ] Week 3 ê³„íš í™•ì¸
- [ ] LangChain ê°œë… ì˜ˆìŠµ
- [ ] OpenAI API í‚¤ ë°œê¸‰

## ğŸ“Š í•™ìŠµ ì‹œê°„ ê¸°ë¡

| ì¼ì | í™œë™ | ì‹œê°„ | ì™„ë£Œ |
|------|------|------|------|
| Day 8 | CNN ê°•ì˜ + Transfer Learning | 8h | [ ] |
| Day 9 | ë¬¸ì„œ ë¶„ë¥˜ í”„ë¡œì íŠ¸ | 8h | [ ] |
| Day 10 | LSTM ì£¼ê°€ ì˜ˆì¸¡ | 8h | [ ] |
| Day 11 | Transformers ê°•ì˜ | 8h | [ ] |
| Day 12 | BERT Fine-tuning | 8h | [ ] |
| Day 13 | ê°ì„± ë¶„ì„ í”„ë¡œì íŠ¸ | 8h | [ ] |
| Day 14 | ë¬¸ì„œ Q&A ì™„ì„± | 8h | [ ] |

---

**Week 2 ì™„ë£Œ í›„ â†’ Week 3 (BERT ì‹¬í™”)ë¡œ ì§„í–‰**
