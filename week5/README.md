# Week 5: í†µí•© í”„ë¡œì íŠ¸ + MLOps + í¬íŠ¸í´ë¦¬ì˜¤ (56ì‹œê°„)

> **ëª©í‘œ:** AI ê¸°ë°˜ ê¸ˆìœµ ê·œì œ ì¤€ìˆ˜ ì‹œìŠ¤í…œ ì™„ì„± + í¬íŠ¸í´ë¦¬ì˜¤ ë°°í¬

## ğŸ“… ì£¼ì°¨ ì¼ì •

### Day 29-31 (ì›”-ìˆ˜): ìµœì¢… í”„ë¡œì íŠ¸ í†µí•©
**í•™ìŠµ ì‹œê°„:** 24ì‹œê°„
- BERT + RAG í†µí•©
- FastAPI ë°±ì—”ë“œ
- Streamlit í”„ë¡ íŠ¸ì—”ë“œ

### Day 32-33 (ëª©-ê¸ˆ): MLOps
**í•™ìŠµ ì‹œê°„:** 16ì‹œê°„
- MLflow ì‹¤í—˜ ì¶”ì 
- Docker ì»¨í…Œì´ë„ˆí™”
- docker-compose ë°°í¬

### Day 34-35 (í† -ì¼): í¬íŠ¸í´ë¦¬ì˜¤ ì™„ì„±
**í•™ìŠµ ì‹œê°„:** 16ì‹œê°„
- GitHub ì €ì¥ì†Œ ì •ë¦¬
- README & ë¬¸ì„œí™”
- ê¸°ìˆ  ë¸”ë¡œê·¸ ì‘ì„±

## ğŸ¯ ìµœì¢… ëª©í‘œ

### ì™„ì„± í”„ë¡œì íŠ¸
**AI ê¸°ë°˜ ê¸ˆìœµ ê·œì œ ì¤€ìˆ˜ ì‹œìŠ¤í…œ**
- ë¬¸ì„œ ë¶„ë¥˜ (BERT)
- RAG ê²€ìƒ‰ (LangChain)
- ê·œì œ ìœ„ë°˜ ë¶„ì„ (GPT-4)
- ì›¹ ì¸í„°í˜ì´ìŠ¤ (Streamlit)
- Docker ë°°í¬

### í¬íŠ¸í´ë¦¬ì˜¤
- GitHub ì €ì¥ì†Œ 3ê°œ
- ê¸°ìˆ  ë¸”ë¡œê·¸ 2í¸
- í”„ë¡œì íŠ¸ ë°ëª¨ ì˜ìƒ

## ğŸ—ï¸ ìµœì¢… í”„ë¡œì íŠ¸: AI ê¸°ë°˜ ê¸ˆìœµ ê·œì œ ì¤€ìˆ˜ ì‹œìŠ¤í…œ

### ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Frontend (Streamlit)                  â”‚
â”‚  - ë¬¸ì„œ ì—…ë¡œë“œ                                 â”‚
â”‚  - Q&A ì¸í„°í˜ì´ìŠ¤                              â”‚
â”‚  - ëŒ€ì‹œë³´ë“œ                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚ HTTP/REST
                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         FastAPI Backend                       â”‚
â”‚  - /upload    (ë¬¸ì„œ ì—…ë¡œë“œ)                    â”‚
â”‚  - /query     (RAG ê²€ìƒ‰)                       â”‚
â”‚  - /classify  (ë¬¸ì„œ ë¶„ë¥˜)                      â”‚
â”‚  - /analyze   (ê·œì œ ìœ„ë°˜ ë¶„ì„)                 â”‚
â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”˜
   â”‚        â”‚         â”‚                      â”‚
   â–¼        â–¼         â–¼                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚LangChainâ”‚ â”‚PyTorchâ”‚ â”‚ ChromaDB â”‚ â”‚   MLflow    â”‚
â”‚+ OpenAI â”‚ â”‚ BERT  â”‚ â”‚Vector DB â”‚ â”‚ì‹¤í—˜ ì¶”ì      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### í”„ë¡œì íŠ¸ êµ¬ì¡°

```
final_project/
â”œâ”€â”€ README.md
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env.example
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py                    # FastAPI ì„œë²„
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ classifier_service.py  # BERT ë¶„ë¥˜
â”‚   â”‚   â”œâ”€â”€ rag_service.py         # RAG ê²€ìƒ‰
â”‚   â”‚   â””â”€â”€ compliance_analyzer.py # ê·œì œ ë¶„ì„
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ document_processor.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ app.py                     # Streamlit ì•±
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ mlops/
â”‚   â”œâ”€â”€ train_classifier.py       # ëª¨ë¸ í•™ìŠµ
â”‚   â”œâ”€â”€ evaluate.py                # ëª¨ë¸ í‰ê°€
â”‚   â””â”€â”€ mlflow_tracking.py         # ì‹¤í—˜ ì¶”ì 
â”‚
â””â”€â”€ models/
    â””â”€â”€ doc_classifier/            # í•™ìŠµëœ ëª¨ë¸
```

## ğŸ’» êµ¬í˜„

### 1. ë¬¸ì„œ ë¶„ë¥˜ ì„œë¹„ìŠ¤ (backend/models/classifier_service.py)

```python
"""
BERT ê¸°ë°˜ ë¬¸ì„œ ë¶„ë¥˜ ì„œë¹„ìŠ¤
"""

import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

class DocumentClassifier:
    def __init__(self, model_path="./models/doc_classifier"):
        self.device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
        self.tokenizer = AutoTokenizer.from_pretrained("klue/bert-base")
        self.model = AutoModelForSequenceClassification.from_pretrained(model_path)
        self.model.to(self.device)
        self.model.eval()
        
        self.categories = [
            "ëŒ€ì¶œê³„ì•½ì„œ", "ë³´í—˜ì•½ê´€", "íˆ¬ìì„¤ëª…ì„œ", 
            "ê¸ˆìœµê·œì œë¬¸ì„œ", "ê¸°íƒ€"
        ]
    
    def classify(self, text: str) -> dict:
        """í…ìŠ¤íŠ¸ ë¶„ë¥˜"""
        inputs = self.tokenizer(
            text, 
            return_tensors="pt",
            truncation=True,
            max_length=512
        ).to(self.device)
        
        with torch.no_grad():
            outputs = self.model(**inputs)
            probs = torch.softmax(outputs.logits, dim=1)
            pred_class = torch.argmax(probs, dim=1).item()
        
        return {
            "category": self.categories[pred_class],
            "confidence": float(probs[0][pred_class].item()),
            "all_probabilities": {
                cat: float(prob) 
                for cat, prob in zip(self.categories, probs[0].tolist())
            }
        }
```

### 2. ê·œì œ ì¤€ìˆ˜ ë¶„ì„ê¸° (backend/models/compliance_analyzer.py)

```python
"""
GPT-4 ê¸°ë°˜ ê·œì œ ìœ„ë°˜ ë¶„ì„
"""

from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
import json
import os

class ComplianceAnalyzer:
    def __init__(self):
        self.llm = ChatOpenAI(
            temperature=0,
            model="gpt-4",
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )
        
        self.prompt = PromptTemplate(
            input_variables=["document_type", "content"],
            template="""
            ë‹¹ì‹ ì€ ê¸ˆìœµ ê·œì œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
            
            ë‹¤ìŒ {document_type} ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ ê¸ˆìœµê·œì œ ìœ„ë°˜ ê°€ëŠ¥ì„±ì„ ê²€í† í•˜ì„¸ìš”.
            
            ë¬¸ì„œ ë‚´ìš©:
            {content}
            
            ê²€í†  í•­ëª©:
            1. ê¸ˆìœµì†Œë¹„ìë³´í˜¸ë²• ì¤€ìˆ˜ ì—¬ë¶€
            2. ì •ë³´ ê³µê°œ ì˜ë¬´ ì¶©ì¡± ì—¬ë¶€
            3. ë¶ˆê³µì • ì¡°í•­ ì¡´ì¬ ì—¬ë¶€
            
            JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜:
            {{
              "risk_level": "high/medium/low",
              "violations": ["ìœ„ë°˜ ì‚¬í•­ 1", "ìœ„ë°˜ ì‚¬í•­ 2", ...],
              "recommendations": ["ê¶Œì¥ ì‚¬í•­ 1", "ê¶Œì¥ ì‚¬í•­ 2", ...]
            }}
            
            JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”.
            """
        )
        
        self.chain = LLMChain(llm=self.llm, prompt=self.prompt)
    
    def analyze(self, document_type: str, content: str) -> dict:
        """ê·œì œ ìœ„ë°˜ ë¶„ì„"""
        try:
            # í† í° ì œí•œì„ ìœ„í•´ content ìë¥´ê¸°
            content = content[:4000]
            
            result = self.chain.run(
                document_type=document_type,
                content=content
            )
            
            # JSON íŒŒì‹±
            # GPT-4ê°€ ```json ... ``` í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•  ìˆ˜ ìˆìŒ
            result = result.strip()
            if result.startswith("```json"):
                result = result[7:]
            if result.endswith("```"):
                result = result[:-3]
            
            return json.loads(result.strip())
        
        except Exception as e:
            return {
                "risk_level": "unknown",
                "violations": [f"ë¶„ì„ ì˜¤ë¥˜: {str(e)}"],
                "recommendations": []
            }
```

### 3. FastAPI ë©”ì¸ ì„œë²„ (backend/main.py)

```python
"""
FastAPI í†µí•© ì„œë²„
"""

from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from models.classifier_service import DocumentClassifier
from models.rag_service import EnterpriseRAG
from models.compliance_analyzer import ComplianceAnalyzer
from utils.document_processor import extract_text
import os
from dotenv import load_dotenv

load_dotenv()

app = FastAPI(title="Financial Compliance AI System")

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
classifier = DocumentClassifier()
rag = EnterpriseRAG()
analyzer = ComplianceAnalyzer()

# Request Models
class QueryRequest(BaseModel):
    question: str

# Endpoints

@app.get("/")
async def root():
    return {"message": "Financial Compliance AI API", "version": "1.0"}

@app.post("/api/analyze-document")
async def analyze_document(file: UploadFile = File(...)):
    """ì¢…í•© ë¬¸ì„œ ë¶„ì„"""
    try:
        # 1. íŒŒì¼ ì½ê¸°
        content = await file.read()
        text = extract_text(content, file.filename)
        
        # 2. ë¬¸ì„œ ë¶„ë¥˜
        classification = classifier.classify(text)
        
        # 3. Vector DB ì €ì¥
        num_chunks = rag.add_documents([file.filename])
        
        # 4. ê·œì œ ìœ„ë°˜ ë¶„ì„
        compliance = analyzer.analyze(
            classification['category'],
            text
        )
        
        return {
            "filename": file.filename,
            "classification": classification,
            "compliance_analysis": compliance,
            "chunks_added": num_chunks
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/query")
async def query_rag(request: QueryRequest):
    """RAG ê²€ìƒ‰"""
    result = rag.query(request.question)
    return result

@app.get("/api/health")
async def health_check():
    return {"status": "healthy"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### 4. Streamlit Frontend (frontend/app.py)

```python
"""
Streamlit ì›¹ ì¸í„°í˜ì´ìŠ¤
"""

import streamlit as st
import requests
import os

st.set_page_config(
    page_title="AI ê¸ˆìœµ ê·œì œ ì¤€ìˆ˜ ì‹œìŠ¤í…œ",
    page_icon="ğŸ¦",
    layout="wide"
)

st.title("ğŸ¦ AI ê¸°ë°˜ ê¸ˆìœµ ê·œì œ ì¤€ìˆ˜ ì‹œìŠ¤í…œ")
st.markdown("---")

# Sidebar
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    backend_url = st.text_input(
        "Backend URL",
        value="http://localhost:8000"
    )
    st.markdown("---")
    st.markdown("### ğŸ“Š ì‹œìŠ¤í…œ ì •ë³´")
    st.info("BERT ë¬¸ì„œ ë¶„ë¥˜ + GPT-4 ê·œì œ ë¶„ì„ + RAG ê²€ìƒ‰")

# Main Content
tab1, tab2 = st.tabs(["ğŸ“„ ë¬¸ì„œ ë¶„ì„", "ğŸ’¬ Q&A"])

# Tab 1: ë¬¸ì„œ ë¶„ì„
with tab1:
    st.header("ë¬¸ì„œ ì—…ë¡œë“œ ë° ë¶„ì„")
    
    uploaded_file = st.file_uploader(
        "ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš” (PDF, DOCX)",
        type=['pdf', 'docx']
    )
    
    if uploaded_file:
        if st.button("ë¶„ì„ ì‹œì‘", type="primary"):
            with st.spinner("ë¶„ì„ ì¤‘..."):
                try:
                    # API í˜¸ì¶œ
                    files = {'file': uploaded_file}
                    response = requests.post(
                        f"{backend_url}/api/analyze-document",
                        files=files
                    )
                    result = response.json()
                    
                    # ê²°ê³¼ í‘œì‹œ
                    st.success("ë¶„ì„ ì™„ë£Œ!")
                    
                    # ë¬¸ì„œ ë¶„ë¥˜
                    col1, col2 = st.columns(2)
                    with col1:
                        st.metric(
                            "ë¬¸ì„œ ë¶„ë¥˜",
                            result['classification']['category']
                        )
                    with col2:
                        st.metric(
                            "ë¶„ë¥˜ ì‹ ë¢°ë„",
                            f"{result['classification']['confidence']:.2%}"
                        )
                    
                    # ê·œì œ ìœ„ë°˜ ë¶„ì„
                    st.subheader("âš ï¸ ê·œì œ ì¤€ìˆ˜ ë¶„ì„")
                    compliance = result['compliance_analysis']
                    
                    # ìœ„í—˜ë„
                    risk_color = {
                        'high': 'ğŸ”´',
                        'medium': 'ğŸŸ¡',
                        'low': 'ğŸŸ¢'
                    }
                    st.write(f"### ìœ„í—˜ë„: {risk_color.get(compliance['risk_level'], 'âšª')} {compliance['risk_level'].upper()}")
                    
                    # ìœ„ë°˜ ì‚¬í•­
                    if compliance['violations']:
                        st.write("**ìœ„ë°˜ ê°€ëŠ¥ì„±:**")
                        for v in compliance['violations']:
                            st.warning(v)
                    
                    # ê¶Œì¥ì‚¬í•­
                    if compliance['recommendations']:
                        st.write("**ê°œì„  ê¶Œì¥ì‚¬í•­:**")
                        for r in compliance['recommendations']:
                            st.info(r)
                
                except Exception as e:
                    st.error(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

# Tab 2: Q&A
with tab2:
    st.header("ë¬¸ì„œ ì§ˆì˜ì‘ë‹µ (RAG)")
    
    question = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")
    
    if st.button("ì§ˆë¬¸í•˜ê¸°"):
        if question:
            with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
                try:
                    response = requests.post(
                        f"{backend_url}/api/query",
                        json={"question": question}
                    )
                    result = response.json()
                    
                    # ë‹µë³€
                    st.write("### ğŸ’¡ ë‹µë³€:")
                    st.write(result['answer'])
                    
                    # ì¶œì²˜
                    with st.expander("ğŸ“š ì¶œì²˜ ë³´ê¸°"):
                        for i, source in enumerate(result['sources'], 1):
                            st.write(f"**Source {i}:**")
                            st.write(source['content'])
                            st.write(f"*Metadata:* {source['metadata']}")
                            st.markdown("---")
                
                except Exception as e:
                    st.error(f"ì˜¤ë¥˜: {str(e)}")
        else:
            st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”")

# Footer
st.markdown("---")
st.markdown("**ê°œë°œ:** YDK | **ê¸°ìˆ  ìŠ¤íƒ:** BERT, GPT-4, LangChain, ChromaDB")
```

### 5. Docker Compose (docker-compose.yml)

```yaml
version: '3.8'

services:
  backend:
    build: ./backend
    ports:
      - "8000:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    volumes:
      - ./models:/app/models
      - ./chroma_db:/app/chroma_db
    command: uvicorn main:app --host 0.0.0.0 --port 8000
  
  frontend:
    build: ./frontend
    ports:
      - "8501:8501"
    depends_on:
      - backend
    environment:
      - BACKEND_URL=http://backend:8000
    command: streamlit run app.py
  
  mlflow:
    image: ghcr.io/mlflow/mlflow:latest
    ports:
      - "5000:5000"
    volumes:
      - ./mlruns:/mlflow
    command: mlflow server --host 0.0.0.0 --port 5000
```

### 6. MLflow ì‹¤í—˜ ì¶”ì  (mlops/train_classifier.py)

```python
"""
MLflowë¥¼ ì‚¬ìš©í•œ ëª¨ë¸ í•™ìŠµ ì¶”ì 
"""

import mlflow
import mlflow.pytorch
from transformers import Trainer, TrainingArguments
import torch

# MLflow ì‹¤í—˜ ì„¤ì •
mlflow.set_experiment("financial_doc_classifier")

with mlflow.start_run(run_name="bert_finetuning_v1"):
    # í•˜ì´í¼íŒŒë¼ë¯¸í„°
    params = {
        "model": "klue/bert-base",
        "learning_rate": 2e-5,
        "batch_size": 16,
        "epochs": 3,
        "max_length": 512
    }
    mlflow.log_params(params)
    
    # í•™ìŠµ
    training_args = TrainingArguments(
        output_dir="./results",
        num_train_epochs=params['epochs'],
        per_device_train_batch_size=params['batch_size'],
        learning_rate=params['learning_rate'],
    )
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
    )
    
    # í•™ìŠµ ì‹¤í–‰
    trainer.train()
    
    # í‰ê°€
    results = trainer.evaluate()
    
    # ë©”íŠ¸ë¦­ ë¡œê¹…
    mlflow.log_metrics(results)
    
    # ëª¨ë¸ ì €ì¥
    mlflow.pytorch.log_model(model, "model")
    
    print(f"Model saved with run_id: {mlflow.active_run().info.run_id}")
```

## âœ… Week 5 ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Day 29-31 (í†µí•© í”„ë¡œì íŠ¸)
- [ ] BERT + RAG í†µí•©
- [ ] FastAPI ë°±ì—”ë“œ ì™„ì„±
- [ ] Streamlit í”„ë¡ íŠ¸ì—”ë“œ ì™„ì„±
- [ ] ì¢…ë‹¨ê°„ í…ŒìŠ¤íŠ¸

### Day 32-33 (MLOps)
- [ ] MLflow ì‹¤í—˜ ì¶”ì 
- [ ] Docker ì´ë¯¸ì§€ ë¹Œë“œ
- [ ] docker-compose ë°°í¬
- [ ] ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

### Day 34-35 (í¬íŠ¸í´ë¦¬ì˜¤)
- [ ] GitHub 3ê°œ ì €ì¥ì†Œ ì •ë¦¬
  - financial-ai-compliance-system
  - pytorch-financial-models
  - langchain-rag-examples
- [ ] README ì‘ì„± (í”„ë¡œ ìˆ˜ì¤€)
- [ ] ê¸°ìˆ  ë¸”ë¡œê·¸ 2í¸
  - "BERT Fine-tuning ì‹¤ì „"
  - "ì—”í„°í”„ë¼ì´ì¦ˆ RAG êµ¬ì¶•ê¸°"
- [ ] ë°ëª¨ ì˜ìƒ ì œì‘

## ğŸ“Š ìµœì¢… ì ê²€

### ê¸°ìˆ  ìŠ¤íƒ ë§ˆìŠ¤í„°
- [ ] Python âœ…
- [ ] PyTorch âœ…
- [ ] BERT âœ…
- [ ] LangChain âœ…
- [ ] FastAPI âœ…
- [ ] Docker âœ…

### í”„ë¡œì íŠ¸ í¬íŠ¸í´ë¦¬ì˜¤
- [ ] 10ê°œ ì‹¤ìŠµ í”„ë¡œì íŠ¸
- [ ] 1ê°œ í†µí•© í”„ë¡œì íŠ¸
- [ ] GitHub ì €ì¥ì†Œ
- [ ] ê¸°ìˆ  ë¸”ë¡œê·¸

## ğŸ“ 5ì£¼ í•™ìŠµ ì™„ë£Œ!

**ì¶•í•˜í•©ë‹ˆë‹¤! ğŸ‰**

280ì‹œê°„ì˜ ì§‘ì¤‘ í•™ìŠµì„ ì™„ë£Œí•˜ì…¨ìŠµë‹ˆë‹¤!

### ë‹¬ì„±í•œ ì—­ëŸ‰
âœ… PyTorch ë”¥ëŸ¬ë‹ ê°œë°œ
âœ… BERT Fine-tuning
âœ… LangChain RAG ì‹œìŠ¤í…œ
âœ… ì—”í„°í”„ë¼ì´ì¦ˆ AI ì‹œìŠ¤í…œ êµ¬ì¶•
âœ… MLOps ê¸°ì´ˆ

### ë‹¤ìŒ ë‹¨ê³„
1. ë©´ì ‘ ì¤€ë¹„
2. ì±„ìš© ê³µê³  ì§€ì›
3. í¬íŠ¸í´ë¦¬ì˜¤ ê³µìœ 

---

**ë‹¹ì‹ ì€ ì´ì œ Enterprise AI System Developerì…ë‹ˆë‹¤! ğŸ’ª**
