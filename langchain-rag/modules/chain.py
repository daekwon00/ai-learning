"""QA ì²´ì¸ ëª¨ë“ˆ

rag-project ëŒ€ì‘: app/api/chat/route.ts
- ê¸°ë³¸ QA: create_retrieval_chain + create_stuff_documents_chain
- ëŒ€í™”í˜• RAG: create_history_aware_retriever (ë©€í‹°í„´ ëŒ€í™” ì§€ì›)
- ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸: rag-projectì˜ ì¶œì²˜ í‘œì‹œ íŒ¨í„´ ìœ ì§€
"""

from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
from langchain_openai import ChatOpenAI
from langchain_chroma import Chroma
from langchain_classic.chains import create_retrieval_chain, create_history_aware_retriever
from langchain_classic.chains.combine_documents import create_stuff_documents_chain

# rag-projectì˜ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ ìœ ì‚¬í•œ íŒ¨í„´
QA_SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ë¬¸ì„œ ê¸°ë°˜ Q&A ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì•„ë˜ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

ê·œì¹™:
1. ë°˜ë“œì‹œ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì— ê¸°ë°˜í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
2. ë‹µë³€ í›„ ì°¸ê³ í•œ ë¬¸ì„œ ì¶œì²˜ë¥¼ í‘œì‹œí•˜ì„¸ìš”.
3. ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ "ì œê³µëœ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µí•˜ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context}"""

# ëŒ€í™”í˜• RAGì—ì„œ ì´ì „ ëŒ€í™”ë¥¼ ê³ ë ¤í•œ ì§ˆë¬¸ ì¬ì‘ì„± í”„ë¡¬í”„íŠ¸
CONTEXTUALIZE_PROMPT = """ì£¼ì–´ì§„ ëŒ€í™” ê¸°ë¡ê³¼ ìµœì‹  ì‚¬ìš©ì ì§ˆë¬¸ì„ ë³´ê³ ,
ëŒ€í™” ê¸°ë¡ì˜ ë§¥ë½ì„ ì°¸ì¡°í•´ì•¼ í•˜ëŠ” ê²½ìš° ë…ë¦½ì ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸ìœ¼ë¡œ ì¬ì‘ì„±í•˜ì„¸ìš”.
ì§ˆë¬¸ì— ë‹µë³€í•˜ì§€ ë§ê³ , í•„ìš”í•˜ë©´ ì¬ì‘ì„±ë§Œ í•˜ì„¸ìš”. ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜í•˜ì„¸ìš”."""


def _get_llm(model: str = "gpt-4o", temperature: float = 0) -> ChatOpenAI:
    """ChatOpenAI ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜í•œë‹¤."""
    return ChatOpenAI(model=model, temperature=temperature)


def create_qa_chain(vectorstore: Chroma, model: str = "gpt-4o"):
    """ê¸°ë³¸ QA ì²´ì¸ì„ ìƒì„±í•œë‹¤.

    rag-projectì˜ ë‹¨ì¼ ì§ˆë¬¸-ì‘ë‹µ íŒ¨í„´ ëŒ€ì‘.
    create_retrieval_chainìœ¼ë¡œ retriever + stuff_documents ê²°í•©.

    Args:
        vectorstore: Chroma ì¸ìŠ¤í„´ìŠ¤
        model: OpenAI ëª¨ë¸ëª…

    Returns:
        Runnable ì²´ì¸ (input: {"input": str} â†’ output: {"answer": str, "context": [Document]})
    """
    llm = _get_llm(model)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

    qa_prompt = ChatPromptTemplate.from_messages([
        ("system", QA_SYSTEM_PROMPT),
        ("human", "{input}"),
    ])

    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    return create_retrieval_chain(retriever, question_answer_chain)


def create_conversational_chain(vectorstore: Chroma, model: str = "gpt-4o"):
    """ëŒ€í™”í˜• RAG ì²´ì¸ì„ ìƒì„±í•œë‹¤.

    rag-projectì—ëŠ” ì—†ëŠ” í™•ì¥ ê¸°ëŠ¥: ë©€í‹°í„´ ëŒ€í™”ì—ì„œ ì´ì „ ë§¥ë½ì„ ê³ ë ¤.
    history_aware_retrieverë¡œ ì§ˆë¬¸ì„ ì¬ì‘ì„±í•œ í›„ ê²€ìƒ‰.

    Args:
        vectorstore: Chroma ì¸ìŠ¤í„´ìŠ¤
        model: OpenAI ëª¨ë¸ëª…

    Returns:
        Runnable ì²´ì¸ (input: {"input": str, "chat_history": list} â†’ output: {"answer": str})
    """
    llm = _get_llm(model)
    retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

    # Step 1: ëŒ€í™” ë§¥ë½ì„ ê³ ë ¤í•œ ì§ˆë¬¸ ì¬ì‘ì„±
    contextualize_prompt = ChatPromptTemplate.from_messages([
        ("system", CONTEXTUALIZE_PROMPT),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ])
    history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_prompt)

    # Step 2: QA ì²´ì¸
    qa_prompt = ChatPromptTemplate.from_messages([
        ("system", QA_SYSTEM_PROMPT),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ])
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)

    return create_retrieval_chain(history_aware_retriever, question_answer_chain)


def ask(chain, question: str, chat_history: list | None = None) -> dict:
    """ì²´ì¸ì— ì§ˆë¬¸í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•œë‹¤.

    Args:
        chain: QA ì²´ì¸ ë˜ëŠ” ëŒ€í™”í˜• RAG ì²´ì¸
        question: ì§ˆë¬¸
        chat_history: ì´ì „ ëŒ€í™” ê¸°ë¡ (ëŒ€í™”í˜• RAGì—ì„œë§Œ ì‚¬ìš©)

    Returns:
        {"answer": str, "context": list[Document], "chat_history": list}
    """
    input_data = {"input": question}
    if chat_history is not None:
        input_data["chat_history"] = chat_history
    else:
        chat_history = []

    result = chain.invoke(input_data)

    print(f"\n{'='*60}")
    print(f"Q: {question}")
    print(f"{'='*60}")
    print(f"A: {result['answer']}")

    if result.get("context"):
        print(f"\nğŸ“„ ì°¸ì¡° ë¬¸ì„œ ({len(result['context'])}ê°œ):")
        sources = set()
        for doc in result["context"]:
            src = doc.metadata.get("source", "N/A")
            if src not in sources:
                sources.add(src)
                print(f"  - {src}")

    # ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸
    updated_history = chat_history + [
        HumanMessage(content=question),
        AIMessage(content=result["answer"]),
    ]

    return {
        "answer": result["answer"],
        "context": result.get("context", []),
        "chat_history": updated_history,
    }


if __name__ == "__main__":
    import os
    from dotenv import load_dotenv

    load_dotenv(os.path.join(os.path.dirname(__file__), "..", ".env"))

    from vectorstore import get_vectorstore

    vs = get_vectorstore()
    chain = create_qa_chain(vs)

    ask(chain, "RAGë€ ë¬´ì—‡ì¸ê°€ìš”?")
    ask(chain, "Next.jsì˜ ì£¼ìš” íŠ¹ì§•ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.")
